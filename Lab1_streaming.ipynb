{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXWbXc6zfkZCgec8XmJLF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jalalbaim/AI-Powered-X-Ray-Image-Analysis-for-Continuous-Tomography-System-Monitoring/blob/main/Lab1_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Data stream Processing\n",
        "@author: MJ.Baim\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6NHrx0VLgGC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## River Framework"
      ],
      "metadata": {
        "id": "1po0CaICgcNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install river"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDQEygetgYlo",
        "outputId": "0f56fd0c-cb19-4dde-bba6-243a16d38b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting river\n",
            "  Downloading river-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from river) (2.0.2)\n",
            "Collecting pandas<3.0.0,>=2.2.3 (from river)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from river) (1.16.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.3->river) (1.17.0)\n",
            "Downloading river-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m140.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, river\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3 river-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from river import datasets, preprocessing, linear_model, metrics, optim, forest, drift, tree\n",
        "\n",
        "def mean_ms(total_s, n):\n",
        "    return (total_s / max(n, 1)) * 1_000\n",
        "\n",
        "def print_header(title):\n",
        "    print(\"\\n\" + \"=\"*len(title))\n",
        "    print(title)\n",
        "    print(\"=\"*len(title))\n",
        "\n",
        "# -----------------------------\n",
        "# ELEC2 — Classification with drift\n",
        "# Modèles:\n",
        "#   1) Baseline = StandardScaler + LogisticRegression + reset sur drift ADWIN (externe)\n",
        "#   2) ARF = Adaptive Random Forest (pas de reset externe)\n",
        "#   3) HAT = Hoeffding Adaptive Tree (pas de reset externe)\n",
        "#   4) EFDT = Extremely Fast Decision Tree (pas de reset externe)\n",
        "# Métriques: Acc, F1, Kappa, Balanced Acc, temps/élément (ms), flags/reset ADWIN\n",
        "# -----------------------------\n",
        "def run_elec2(max_n=None, report_every=20000):\n",
        "    ds = datasets.Elec2()\n",
        "    n = 0\n",
        "\n",
        "    # 1) Baseline\n",
        "    def new_base():\n",
        "        return preprocessing.StandardScaler() | linear_model.LogisticRegression(\n",
        "            optimizer=optim.SGD(0.01), l2=1e-3\n",
        "        )\n",
        "    base_model = new_base()\n",
        "\n",
        "    # 2) ARF\n",
        "    arf = forest.ARFClassifier(n_models=10, seed=42)\n",
        "\n",
        "    # 3) HAT\n",
        "    hat = tree.HoeffdingAdaptiveTreeClassifier(seed=42)\n",
        "\n",
        "    # 4) EFDT\n",
        "    efdt = tree.ExtremelyFastDecisionTreeClassifier()\n",
        "\n",
        "    # Metrics (une famille par modèle)\n",
        "    acc_base, f1_base = metrics.Accuracy(), metrics.F1()\n",
        "    kap_base, bal_base = metrics.CohenKappa(), metrics.BalancedAccuracy()\n",
        "\n",
        "    acc_arf, f1_arf = metrics.Accuracy(), metrics.F1()\n",
        "    kap_arf, bal_arf = metrics.CohenKappa(), metrics.BalancedAccuracy()\n",
        "\n",
        "    acc_hat, f1_hat = metrics.Accuracy(), metrics.F1()\n",
        "    kap_hat, bal_hat = metrics.CohenKappa(), metrics.BalancedAccuracy()\n",
        "\n",
        "    acc_efdt, f1_efdt = metrics.Accuracy(), metrics.F1()\n",
        "    kap_efdt, bal_efdt = metrics.CohenKappa(), metrics.BalancedAccuracy()\n",
        "\n",
        "    # Timings\n",
        "    t_base = 0.0\n",
        "    t_arf  = 0.0\n",
        "    t_hat  = 0.0\n",
        "    t_efdt = 0.0\n",
        "\n",
        "    # Détecteurs de drift sur flux d'erreurs (mesure)\n",
        "    adw_base = drift.ADWIN()\n",
        "    adw_arf  = drift.ADWIN()\n",
        "    adw_hat  = drift.ADWIN()\n",
        "    adw_efdt = drift.ADWIN()\n",
        "\n",
        "    # Compteurs de détection (reset réel uniquement pour baseline)\n",
        "    drift_resets_base = 0\n",
        "    drift_flags_arf   = 0\n",
        "    drift_flags_hat   = 0\n",
        "    drift_flags_efdt  = 0\n",
        "\n",
        "    # Courbes d'apprentissage (checkpoints)\n",
        "    checkpoints = []\n",
        "\n",
        "    for x, y in ds:\n",
        "        if (max_n is not None) and (n >= max_n):\n",
        "            break\n",
        "        n += 1\n",
        "\n",
        "        # -------- Baseline --------\n",
        "        t0 = time.perf_counter()\n",
        "        yb = base_model.predict_one(x)\n",
        "        acc_base.update(y_true=y, y_pred=yb)\n",
        "        f1_base.update(y_true=y, y_pred=yb)\n",
        "        kap_base.update(y_true=y, y_pred=yb)\n",
        "        bal_base.update(y_true=y, y_pred=yb)\n",
        "        base_model.learn_one(x, y)\n",
        "        t_base += time.perf_counter() - t0\n",
        "\n",
        "        err_b = 0 if (yb == y) else 1\n",
        "        adw_base.update(err_b)\n",
        "        if adw_base.drift_detected:\n",
        "            drift_resets_base += 1\n",
        "            base_model = new_base()  # reset réel\n",
        "\n",
        "        # -------- ARF --------\n",
        "        t1 = time.perf_counter()\n",
        "        ya = arf.predict_one(x)\n",
        "        acc_arf.update(y_true=y, y_pred=ya)\n",
        "        f1_arf.update(y_true=y, y_pred=ya)\n",
        "        kap_arf.update(y_true=y, y_pred=ya)\n",
        "        bal_arf.update(y_true=y, y_pred=ya)\n",
        "        arf.learn_one(x, y)\n",
        "        t_arf += time.perf_counter() - t1\n",
        "\n",
        "        err_a = 0 if (ya == y) else 1\n",
        "        adw_arf.update(err_a)\n",
        "        if adw_arf.drift_detected:\n",
        "            drift_flags_arf += 1  # mesure (pas de reset)\n",
        "\n",
        "        # -------- HAT --------\n",
        "        t2 = time.perf_counter()\n",
        "        yh = hat.predict_one(x)\n",
        "        acc_hat.update(y_true=y, y_pred=yh)\n",
        "        f1_hat.update(y_true=y, y_pred=yh)\n",
        "        kap_hat.update(y_true=y, y_pred=yh)\n",
        "        bal_hat.update(y_true=y, y_pred=yh)\n",
        "        hat.learn_one(x, y)\n",
        "        t_hat += time.perf_counter() - t2\n",
        "\n",
        "        err_h = 0 if (yh == y) else 1\n",
        "        adw_hat.update(err_h)\n",
        "        if adw_hat.drift_detected:\n",
        "            drift_flags_hat += 1  # mesure (pas de reset)\n",
        "\n",
        "        # -------- EFDT --------\n",
        "        t3 = time.perf_counter()\n",
        "        ye = efdt.predict_one(x)\n",
        "        acc_efdt.update(y_true=y, y_pred=ye)\n",
        "        f1_efdt.update(y_true=y, y_pred=ye)\n",
        "        kap_efdt.update(y_true=y, y_pred=ye)\n",
        "        bal_efdt.update(y_true=y, y_pred=ye)\n",
        "        efdt.learn_one(x, y)\n",
        "        t_efdt += time.perf_counter() - t3\n",
        "\n",
        "        err_e = 0 if (ye == y) else 1\n",
        "        adw_efdt.update(err_e)\n",
        "        if adw_efdt.drift_detected:\n",
        "            drift_flags_efdt += 1  # mesure (pas de reset)\n",
        "\n",
        "        # Checkpoints\n",
        "        if report_every and (n % report_every == 0):\n",
        "            checkpoints.append((\n",
        "                n,\n",
        "                acc_base.get(), f1_base.get(), kap_base.get(), bal_base.get(),\n",
        "                acc_arf.get(),  f1_arf.get(),  kap_arf.get(),  bal_arf.get(),\n",
        "                acc_hat.get(),  f1_hat.get(),  kap_hat.get(),  bal_hat.get(),\n",
        "                acc_efdt.get(), f1_efdt.get(), kap_efdt.get(), bal_efdt.get(),\n",
        "                drift_resets_base, drift_flags_arf, drift_flags_hat, drift_flags_efdt\n",
        "            ))\n",
        "\n",
        "    # -------------------- Rapport final --------------------\n",
        "    print_header(\"ELEC2 — Final\")\n",
        "    print(f\"samples: {n}\")\n",
        "\n",
        "    print(\"\\n[Baseline: Logistic + ADWIN reset]\")\n",
        "    print(f\"Acc={acc_base.get():.4f} | F1={f1_base.get():.4f} | Kappa={kap_base.get():.4f} | \"\n",
        "          f\"BalAcc={bal_base.get():.4f} | time/elem={mean_ms(t_base, n):.3f} ms | resets={drift_resets_base}\")\n",
        "\n",
        "    print(\"\\n[ARF: Adaptive Random Forest]\")\n",
        "    print(f\"Acc={acc_arf.get():.4f} | F1={f1_arf.get():.4f} | Kappa={kap_arf.get():.4f} | \"\n",
        "          f\"BalAcc={bal_arf.get():.4f} | time/elem={mean_ms(t_arf, n):.3f} ms | ADWIN flags={drift_flags_arf}\")\n",
        "\n",
        "    print(\"\\n[HAT: Hoeffding Adaptive Tree]\")\n",
        "    print(f\"Acc={acc_hat.get():.4f} | F1={f1_hat.get():.4f} | Kappa={kap_hat.get():.4f} | \"\n",
        "          f\"BalAcc={bal_hat.get():.4f} | time/elem={mean_ms(t_hat, n):.3f} ms | ADWIN flags={drift_flags_hat}\")\n",
        "\n",
        "    print(\"\\n[EFDT: Extremely Fast Decision Tree]\")\n",
        "    print(f\"Acc={acc_efdt.get():.4f} | F1={f1_efdt.get():.4f} | Kappa={kap_efdt.get():.4f} | \"\n",
        "          f\"BalAcc={bal_efdt.get():.4f} | time/elem={mean_ms(t_efdt, n):.3f} ms | ADWIN flags={drift_flags_efdt}\")\n",
        "\n",
        "    if checkpoints:\n",
        "        print(\"\\nLearning curve checkpoints:\")\n",
        "        print(\"N\\tAcc_b\\tF1_b\\tKap_b\\tBal_b\\tAcc_ARF\\tF1_ARF\\tKap_ARF\\tBal_ARF\\tAcc_HAT\\tF1_HAT\\tKap_HAT\\tBal_HAT\\tAcc_EFDT\\tF1_EFDT\\tKap_EFDT\\tBal_EFDT\\tResets_b\\tFlags_ARF\\tFlags_HAT\\tFlags_EFDT\")\n",
        "        for (N, ab, fb, kb, bb, aa, fa, ka, ba, ah, fh, kh, bh, ae, fe, ke, be, drb, dfa, dfh, dfe) in checkpoints:\n",
        "            print(f\"{N}\\t{ab:.4f}\\t{fb:.4f}\\t{kb:.4f}\\t{bb:.4f}\\t\"\n",
        "                  f\"{aa:.4f}\\t{fa:.4f}\\t{ka:.4f}\\t{ba:.4f}\\t\"\n",
        "                  f\"{ah:.4f}\\t{fh:.4f}\\t{kh:.4f}\\t{bh:.4f}\\t\"\n",
        "                  f\"{ae:.4f}\\t{fe:.4f}\\t{ke:.4f}\\t{be:.4f}\\t\"\n",
        "                  f\"{drb}\\t\\t{dfa}\\t\\t{dfh}\\t\\t{dfe}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_elec2(report_every=20000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vngumip6wzY",
        "outputId": "56e90d86-245a-46b0-be36-b094b19c1d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://maxhalford.github.io/files/datasets/electricity.zip (697.72 KiB)\n",
            "Uncompressing into /root/river_data/Elec2\n",
            "\n",
            "=============\n",
            "ELEC2 — Final\n",
            "=============\n",
            "samples: 45312\n",
            "\n",
            "[Baseline: Logistic + ADWIN reset]\n",
            "Acc=0.8229 | F1=0.7796 | Kappa=0.6323 | BalAcc=0.8117 | time/elem=0.086 ms | resets=32\n",
            "\n",
            "[ARF: Adaptive Random Forest]\n",
            "Acc=0.8884 | F1=0.8668 | Kappa=0.7709 | BalAcc=0.5893 | time/elem=1.496 ms | ADWIN flags=9\n",
            "\n",
            "[HAT: Hoeffding Adaptive Tree]\n",
            "Acc=0.8359 | F1=0.8028 | Kappa=0.6624 | BalAcc=0.5530 | time/elem=0.229 ms | ADWIN flags=49\n",
            "\n",
            "[EFDT: Extremely Fast Decision Tree]\n",
            "Acc=0.8286 | F1=0.7977 | Kappa=0.6490 | BalAcc=0.5495 | time/elem=0.492 ms | ADWIN flags=41\n",
            "\n",
            "Learning curve checkpoints:\n",
            "N\tAcc_b\tF1_b\tKap_b\tBal_b\tAcc_ARF\tF1_ARF\tKap_ARF\tBal_ARF\tAcc_HAT\tF1_HAT\tKap_HAT\tBal_HAT\tAcc_EFDT\tF1_EFDT\tKap_EFDT\tBal_EFDT\tResets_b\tFlags_ARF\tFlags_HAT\tFlags_EFDT\n",
            "20000\t0.8435\t0.8131\t0.6789\t0.8362\t0.8990\t0.8836\t0.7944\t0.5977\t0.8635\t0.8435\t0.7224\t0.5740\t0.8583\t0.8359\t0.7113\t0.5697\t11\t\t3\t\t18\t\t18\n",
            "40000\t0.8209\t0.7739\t0.6265\t0.8084\t0.8874\t0.8636\t0.7677\t0.5881\t0.8359\t0.8008\t0.6614\t0.5527\t0.8242\t0.7901\t0.6389\t0.5462\t27\t\t7\t\t42\t\t37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from river import datasets, preprocessing, linear_model, naive_bayes, tree, metrics, optim\n",
        "\n",
        "# PHISHING — 4 classifieurs supervisés (sans adaptatif)\n",
        "def run_phishing_supervised_v2(max_n=None, threshold=0.5, report_every=20_000):\n",
        "    ds = datasets.Phishing()  # y in {0,1} (1 = phishing)\n",
        "\n",
        "    models = {\n",
        "        \"PassiveAggressive+Std\": preprocessing.StandardScaler() | linear_model.PAClassifier(\n",
        "            C=0.01, mode=1\n",
        "        ),\n",
        "        \"GaussianNB+Std\": preprocessing.StandardScaler() | naive_bayes.GaussianNB(),\n",
        "        \"EFDT\": tree.ExtremelyFastDecisionTreeClassifier(),\n",
        "        \"ComplementNB+OH\": preprocessing.OneHotEncoder() | naive_bayes.ComplementNB(),\n",
        "    }\n",
        "\n",
        "    metrics_map = {\n",
        "        name: {\n",
        "            \"acc\": metrics.Accuracy(),\n",
        "            \"f1\": metrics.F1(),\n",
        "            \"prec\": metrics.Precision(),\n",
        "            \"rec\": metrics.Recall(),\n",
        "            \"roc\": metrics.ROCAUC()   # pas de pos_label dans ta version\n",
        "        }\n",
        "        for name in models\n",
        "    }\n",
        "\n",
        "    times = {name: 0.0 for name in models}\n",
        "    checkpoints, n = [], 0\n",
        "\n",
        "    for x, y in ds:\n",
        "        if (max_n is not None) and (n >= max_n):\n",
        "            break\n",
        "        n += 1\n",
        "\n",
        "        for name, model in models.items():\n",
        "            t0 = time.perf_counter()\n",
        "\n",
        "            # --- prédiction robuste ---\n",
        "            p1 = 0.0\n",
        "            if hasattr(model, \"predict_proba_one\"):\n",
        "                proba = model.predict_proba_one(x)\n",
        "                if isinstance(proba, dict):\n",
        "                    p1 = proba.get(1, 0.0)\n",
        "\n",
        "            # si pas de proba dispo, fallback via predict_one()\n",
        "            if p1 == 0.0 and not hasattr(model, \"predict_proba_one\"):\n",
        "                y_hat_tmp = model.predict_one(x)\n",
        "                p1 = 1.0 if y_hat_tmp == 1 else 0.0\n",
        "\n",
        "            y_hat = 1 if p1 >= threshold else 0\n",
        "\n",
        "            # --- métriques ---\n",
        "            m = metrics_map[name]\n",
        "            m[\"acc\"].update(y, y_hat)\n",
        "            m[\"f1\"].update(y, y_hat)\n",
        "            m[\"prec\"].update(y, y_hat)\n",
        "            m[\"rec\"].update(y, y_hat)\n",
        "            m[\"roc\"].update(y, p1)\n",
        "\n",
        "            # --- apprentissage (NE PAS réassigner) ---\n",
        "            model.learn_one(x, y)\n",
        "\n",
        "            times[name] += time.perf_counter() - t0\n",
        "\n",
        "        if report_every and (n % report_every == 0):\n",
        "            checkpoints.append((\n",
        "                n,\n",
        "                *[metrics_map[k][\"acc\"].get() for k in models],\n",
        "                *[metrics_map[k][\"f1\"].get() for k in models],\n",
        "                *[metrics_map[k][\"roc\"].get() for k in models],\n",
        "            ))\n",
        "\n",
        "    print(\"\\n=========== PHISHING — Final (v2, fixed) ===========\")\n",
        "    print(f\"samples: {n}\")\n",
        "    for name in models:\n",
        "        m = metrics_map[name]\n",
        "        ms_per_elem = (times[name] / max(n, 1)) * 1000\n",
        "        print(f\"\\n[{name}]\")\n",
        "        print(f\"Acc={m['acc'].get():.4f} | F1={m['f1'].get():.4f} | \"\n",
        "              f\"Prec={m['prec'].get():.4f} | Rec={m['rec'].get():.4f} | \"\n",
        "              f\"ROC-AUC={m['roc'].get():.4f} | time/elem={ms_per_elem:.3f} ms\")\n",
        "\n",
        "    if checkpoints:\n",
        "        print(\"\\nLearning curve checkpoints:\")\n",
        "        heads = [\"N\"] + [f\"Acc_{k}\" for k in models] + [f\"F1_{k}\" for k in models] + [f\"AUC_{k}\" for k in models]\n",
        "        print(\"\\t\".join(heads))\n",
        "        for row in checkpoints:\n",
        "            print(\"\\t\".join([str(row[0])] + [f\"{v:.4f}\" for v in row[1:]]))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_phishing_supervised_v2(max_n=100_000, report_every=20_000)\n"
      ],
      "metadata": {
        "id": "rCARFpIg563e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df11614-12db-4cb2-f2c1-e8d2e08a65e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========== PHISHING — Final (v2, fixed) ===========\n",
            "samples: 1250\n",
            "\n",
            "[PassiveAggressive+Std]\n",
            "Acc=0.8968 | F1=0.8833 | Prec=0.8761 | Rec=0.8905 | ROC-AUC=0.9552 | time/elem=0.078 ms\n",
            "\n",
            "[GaussianNB+Std]\n",
            "Acc=0.8792 | F1=0.8660 | Prec=0.8428 | Rec=0.8905 | ROC-AUC=0.9173 | time/elem=0.238 ms\n",
            "\n",
            "[EFDT]\n",
            "Acc=0.8872 | F1=0.8726 | Prec=0.8640 | Rec=0.8814 | ROC-AUC=0.9082 | time/elem=0.785 ms\n",
            "\n",
            "[ComplementNB+OH]\n",
            "Acc=0.9048 | F1=0.8944 | Prec=0.8705 | Rec=0.9197 | ROC-AUC=0.9569 | time/elem=0.381 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CAPYMOA"
      ],
      "metadata": {
        "id": "mDSQuanZhaeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install capymoa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWXwyvHxhc_4",
        "outputId": "0c0ab832-b590-4096-bf5f-f24619cceb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting capymoa\n",
            "  Downloading capymoa-0.11.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from capymoa) (8.3.0)\n",
            "Collecting deprecated (from capymoa)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jpype1>=v1.5.1 (from capymoa)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from capymoa) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from capymoa) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from capymoa) (2.3.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from capymoa) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from capymoa) (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from capymoa) (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from capymoa) (4.67.1)\n",
            "Collecting wget (from capymoa)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from jpype1>=v1.5.1->capymoa) (25.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->capymoa) (1.17.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->capymoa) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->capymoa) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->capymoa) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->capymoa) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->capymoa) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->capymoa) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->capymoa) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->capymoa) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->capymoa) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->capymoa) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->capymoa) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->capymoa) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->capymoa) (1.17.0)\n",
            "Downloading capymoa-0.11.0-py3-none-any.whl (60.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=99958a5eadaf0ceca540efb05cac77c6bdaf2aa3cdbf493c02ab15d0cb116a5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, jpype1, deprecated, capymoa\n",
            "Successfully installed capymoa-0.11.0 deprecated-1.2.18 jpype1-1.6.0 wget-3.2\n"
          ]
        }
      ]
    }
  ]
}